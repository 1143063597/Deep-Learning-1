{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width='100%'>\n",
    "<tr>\n",
    "<td style='background-color:white'>\n",
    "    <p align=\"left\">\n",
    "    Exercises for the course<br>\n",
    "        <b>Deep Learning 1</b><br>\n",
    "    Winter Semester 2022/23\n",
    "    </p>\n",
    "</td>\n",
    "<td style='background-color:white'>\n",
    "    Machine Learning Group<br>\n",
    "    <b>Faculty IV – Electrical Engineering and Computer Science</b><br>\n",
    "    Technische Universität Berlin\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <h1>Exercise Sheet 1-2 (programming part)</h1>\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "In this homework, our goal is to test different approaches to implement neural networks. Here, we will be focusing on programming forward and backward computations. Training neural networks will be done in the next homework. The neural network we consider is depicted below:\n",
    "\n",
    "![](files/net.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementing of Backpropagation (10 P)\n",
    "\n",
    "The following code implements the forward pass of this network in numpy. Here, you are asked to implement the backward pass, and obtain the gradient with respect to the weight and bias parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-07T15:39:55.900796Z",
     "start_time": "2022-11-07T15:39:55.407795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5422821523392451\n"
     ]
    }
   ],
   "source": [
    "import numpy,utils\n",
    "\n",
    "# 1. Get the data and parameters\n",
    "\n",
    "X,T = utils.getdata()\n",
    "W,B = utils.getparams()\n",
    "A = [X]\n",
    "\n",
    "# 2. Run the forward pass\n",
    "for i in range(3): A.append(numpy.maximum(0,A[-1].dot(W[i])+B[i]))\n",
    "Y = A[-1].dot(W[3])+B[3]\n",
    "\n",
    "# 3. Compute the error\n",
    "err = ((Y-T)**2).mean()\n",
    "\n",
    "# 4. Error backpropagation (TODO: replace by your code)\n",
    "# * dE/dY\n",
    "def err_derivative(Y, T):\n",
    "    return 2*(Y - T)\n",
    "\n",
    "# * A_i = max(0, Z_i) => dA_i/dZ_i = 1 if Z_i > 0 else 0\n",
    "def reLu_derivative(A):\n",
    "    return (A > 0)*1\n",
    "def back_propagation(W,B,A,Y,T):\n",
    "    dY = err_derivative(Y,T)\n",
    "    deltas = [dY] # dErr/dY\n",
    "    DW = []\n",
    "    DB = []\n",
    "    for i in reversed(range(len(W))):\n",
    "        if i == len(W)-1 :\n",
    "            dW_i = deltas[-1].dot(A[i]) # dW_4\n",
    "            dB_i = deltas[-1] # dB_4\n",
    "            DW.insert(0,dW_i)\n",
    "            DB.insert(0,dB_i)\n",
    "            dA_i = W[i].dot(deltas[-1])\n",
    "            deltas.insert(0, dA_i)\n",
    "        else:\n",
    "            derivative_activation = reLu_derivative(A[i+1])\n",
    "            dW_i = A[i].T.dot(derivative_activation * deltas[0].T)\n",
    "            DW.insert(0,dW_i)\n",
    "            dB_i = derivative_activation * deltas[0].T\n",
    "            DB.insert(0, dB_i)\n",
    "            dA_i = W[i].dot((derivative_activation * deltas[0].T).T)\n",
    "            deltas.insert(0, dA_i)\n",
    "\n",
    "    return DW\n",
    "DW = back_propagation(W,B,A,Y,T)\n",
    "# 5. Show error gradient w.r.t. the 1st weight parameter\n",
    "print(numpy.linalg.norm(DW[0][0,0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Using Automatic Differentiation (10 P)\n",
    "\n",
    "Because gradient computation can be error-prone, we often rely on libraries that incorporate automatic differentiation. In this exercise, we make use of the PyTorch library. You are then asked to compute the error of the neural network within that framework, which will then be automatically differentiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-07T15:39:57.325794Z",
     "start_time": "2022-11-07T15:39:55.902795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5422821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\87290\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Get the data and parameters\n",
    "\n",
    "X,T = utils.getdata()\n",
    "W,B = utils.getparams()\n",
    "\n",
    "# 2. Convert to PyTorch objects\n",
    "\n",
    "X = torch.Tensor(X)\n",
    "T = torch.Tensor(T)\n",
    "W = [nn.Parameter(torch.Tensor(w)) for w in W]\n",
    "B = [nn.Parameter(torch.Tensor(b)) for b in B]\n",
    "\n",
    "# 3. Compute the forward pass and the error (TODO: replace by your code)\n",
    "input_from_prev = X \n",
    "relu = nn.ReLU()\n",
    "\n",
    "for i in range(3):\n",
    "    output = relu(input_from_prev @ W[i] + B[i])\n",
    "    input_from_prev = output\n",
    "    \n",
    "err = ((input_from_prev @ W[3] + B[3] - T)**2).mean()\n",
    "\n",
    "# 4. Apply automatic differentiation\n",
    "\n",
    "err.backward()\n",
    "\n",
    "# 5. Show error gradient w.r.t. the 1st weight parameter\n",
    "\n",
    "print(numpy.linalg.norm(W[0].grad[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Object-Oriented Implementation (10 P)\n",
    "\n",
    "As a last exercise, we would like to make use of existing neural network objects of the PyTorch library. Here, most of the code is already implemented for you. You are only asked to find where the error gradient of the first weight parameter has been stored, and to print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-07T15:40:17.761892Z",
     "start_time": "2022-11-07T15:40:17.745893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5422821\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1. Get the data and parameters\n",
    "\n",
    "X,T = utils.getdata()\n",
    "W,B = utils.getparams()\n",
    "\n",
    "# 2. Convert to PyTorch objects\n",
    "\n",
    "X = torch.Tensor(X)\n",
    "T = torch.Tensor(T)\n",
    "W = [torch.nn.Parameter(torch.Tensor(w.T)) for w in W]\n",
    "B = [torch.nn.Parameter(torch.Tensor(b)) for b in B]\n",
    "\n",
    "# 3. Build the neural network\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "          nn.Linear(4,6),nn.ReLU(),\n",
    "          nn.Linear(6,6),nn.ReLU(),\n",
    "          nn.Linear(6,6),nn.ReLU(),\n",
    "          nn.Linear(6,1))\n",
    "\n",
    "for l,w,b in zip(list(net)[::2],W,B):\n",
    "    l.weight = w\n",
    "    l.bias = b\n",
    "    \n",
    "# 4. Compute the forward pass and the error gradient\n",
    "\n",
    "Y = net.forward(X)\n",
    "err = ((Y-T)**2).mean()\n",
    "err.backward()\n",
    "\n",
    "# 5. Show error gradient w.r.t. the 1st weight parameter (TODO: replace by your code)\n",
    "print(np.linalg.norm(net[0].weight.grad[0,0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
