{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:10.282574Z",
     "start_time": "2022-12-02T11:38:08.946550Z"
    },
    "id": "BQEfMV0lsnqR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvL99MK-srTJ"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "In the following, we will develop an approach to compute the size of the receptive field of any output neuron from a PyTorch model.\n",
    "\n",
    "We divide the implementation of the approach into several functions, and your task is to implement these functions. Once these steps are implemented, you will use the implementation to calculate the size of the receptive fields in various configurations.\n",
    "\n",
    "\n",
    "\n",
    "**This programming exercise consists of 5 tasks (4 programming and 1 questionaire), and the exercise is worth 45  points in total; the distribution of the points is 10 + 10 + 10 + 10 + 5 Points;** These tasks are labelled as Task 1 to Task 5.\n",
    "<!-- \n",
    "\n",
    "\n",
    "- Assumption 1: gradient is not varnish\n",
    "- Assumption 2: input has 2 spatial dimensions: width and height.  We assume that width = height\n",
    "\n",
    "\n",
    "4 Tasks with 30 = 5 + 10 + 10 + 5 P -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElaryLNqKulf"
   },
   "source": [
    "# Core Implementation of the Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:10.298575Z",
     "start_time": "2022-12-02T11:38:10.284582Z"
    },
    "id": "vm3G24pbuCuq"
   },
   "outputs": [],
   "source": [
    "# We set the seed to make the dummy input below comparable across runs.\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# This is dummy input that we will use throughout the sheet.\n",
    "# Because the approach does NOT need to use actual input, we therefore use random input.\n",
    "# In fact, we use 10 random inputs and aggregate the quantity of interest from these 10 inputs.\n",
    "# This aggregation makes sure that we have a good estimate of the quanity.\n",
    "x = torch.randn((10, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:10.344575Z",
     "start_time": "2022-12-02T11:38:10.300584Z"
    },
    "id": "XBsCtvciKOk0"
   },
   "outputs": [],
   "source": [
    "def calculate_receptive_field_size(forward_func, x):\n",
    "    \"\"\"\n",
    "    This function is the main function that integrates three necessary functions together\n",
    "    to calcuate the size of the receptive field.\n",
    "    \n",
    "    Your task is to implement these functions (defined outside this main function).\n",
    "    To aid the implemenation, we provide a number of assertions in this main function\n",
    "    to aid/verify your solutions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1\n",
    "    dependency = find_dependency_between_output_and_input(forward_func, x)\n",
    "    assert dependency is not None\n",
    "\n",
    "    # sum over channel and batch dimension\n",
    "    dependency = dependency.sum(axis=1).sum(axis=0)\n",
    "\n",
    "    # Step 2\n",
    "    mask = criteria(dependency)\n",
    "    assert mask.shape == dependency.shape \n",
    "\n",
    "    # Step 3\n",
    "    size =  calculate_size(mask)\n",
    "    assert (size * 10) % 10 == 0, \"We should have a whole number.\"\n",
    "\n",
    "    return int(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:10.360602Z",
     "start_time": "2022-12-02T11:38:10.346587Z"
    },
    "id": "8x8bhtCBKUZ9"
   },
   "outputs": [],
   "source": [
    "def _select_output_neuron(z):\n",
    "    batch_size, dimensions, h, w =  z.shape\n",
    "\n",
    "    assert h == w, \"Output's spatial size is square.\"\n",
    "\n",
    "    # Because we are interested in the size of the receptive at that layer,\n",
    "    # we therefore select the neuron in the middle of the feature map.\n",
    "    # This selection also provents the influence of padding on the calculation.\n",
    "    zj = z[:, :, h//2, h//2]\n",
    "\n",
    "    return zj\n",
    "\n",
    "def find_dependency_between_output_and_input(forward_func: torch.nn.Sequential, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Task 1 (10P):\n",
    "    \n",
    "    Given input `x`, compute the quantity of interest from an output neuron from `forward_func`.\n",
    "\n",
    "    Hint: You can use `_select_output_neuron` to select such an output neuron.\n",
    "    \"\"\"\n",
    "    x = x.clone()\n",
    "    x.requires_grad = True\n",
    "    forward = forward_func(x)\n",
    "    selected = _select_output_neuron(forward)\n",
    "    selected = selected.mean()\n",
    "    selected.backward()\n",
    "    return x.grad\n",
    "#     forward = forward_func(x)\n",
    "#     selected = _select_output_neuron(forward)\n",
    "    \n",
    "#     layer_list = []\n",
    "#     for name, module in forward_func.named_modules():\n",
    "#         if isinstance(module, torch.nn.Conv2d):\n",
    "#             # print(name, module.kernel_size, module.padding)\n",
    "#             layer_list.append(torch.nn.ConvTranspose2d(module.out_channels, module.in_channels, kernel_size=module.kernel_size, padding=module.padding, stride=module.stride))\n",
    "#         elif(isinstance(module, torch.nn.MaxPool2d)):\n",
    "#             print(name, module.kernel_size, module.padding)\n",
    "#             # layer_list.append(torch.nn.MaxUnpool2d(kernel_size=module.kernel_size, padding=module.padding))\n",
    "#             layer_list.append(torch.nn.Upsample(scale_factor=module.kernel_size**2, mode='bilinear'))\n",
    "#         elif(isinstance(module, torch.nn.Identity)):\n",
    "#             layer_list.append(torch.nn.Identity())\n",
    "#     layers = torch.nn.Sequential(*layer_list[::-1])\n",
    "#     res = torch.reshape(selected, (selected.shape[0], selected.shape[1], 1, 1))\n",
    "#    return layers(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:10.376605Z",
     "start_time": "2022-12-02T11:38:10.362603Z"
    },
    "id": "qPwK-uxcKWaA"
   },
   "outputs": [],
   "source": [
    "def criteria(inp: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Task 2 (10P):\n",
    "\n",
    "    The function implements a thresholding mechanism that discards spatial entries that the quantity of interest is zero.\n",
    "\n",
    "    Hint: the function should return a tensor whose entires are either zero or one.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    res = torch.where(inp != 0, 1, 0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:10.392600Z",
     "start_time": "2022-12-02T11:38:10.378599Z"
    },
    "id": "FDgaOq-zKZK5"
   },
   "outputs": [],
   "source": [
    "def calculate_size(binary_mask: torch.Tensor) -> int:\n",
    "    \"\"\"\n",
    "    Task 3 (10P):\n",
    "\n",
    "    The function finds the size (e.g., height or width, NOT the area) of the active region in the given binary mask).\n",
    "\n",
    "    Hint: Recall that the shape of the input is square; height and width have the same value.\n",
    "        Leverage this fact and find the size.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert (binary_mask == 1).sum() + (binary_mask == 0).sum() \\\n",
    "        == binary_mask.shape[0] * binary_mask.shape[0], \"We should have a binary mask.\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "#     indices = binary_mask.nonzero()\n",
    "#     size = indices.size(dim=0) ** 0.5\n",
    "\n",
    "    return binary_mask.sum() ** (1/len(binary_mask.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpgUv62otbPG"
   },
   "source": [
    "# Verifying the Implementation on Simple Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first verify our implemenation on simple cases. These cases have two convolution layers with the following configuations\n",
    "## **Case 1**: $(k_1=3, s_1=1)\\rightarrow (k_2=1, s_2=1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:10.408612Z",
     "start_time": "2022-12-02T11:38:10.394605Z"
    },
    "id": "9LoJMxBataJa"
   },
   "outputs": [],
   "source": [
    "simple_model_case_1 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 1, kernel_size=3, stride=1),\n",
    "    torch.nn.Conv2d(1, 1, kernel_size=1, stride=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:10.472612Z",
     "start_time": "2022-12-02T11:38:10.411626Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fYmUR8q3uWKK",
    "outputId": "dc21d5fc-df59-4bb9-a254-fe0bb448a510"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the implementation is correct, we should expect the function outputs `3`.\n",
    "calculate_receptive_field_size(\n",
    "    simple_model_case_1, x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: $(k_1=3, s_1=1)\\rightarrow (k_2=3, s_2=1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:10.488615Z",
     "start_time": "2022-12-02T11:38:10.476632Z"
    },
    "id": "r8pLAOdnBgzS"
   },
   "outputs": [],
   "source": [
    "simple_model_case_2 = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 1, kernel_size=3, stride=1),\n",
    "    torch.nn.Conv2d(1, 1, kernel_size=3, stride=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:10.552619Z",
     "start_time": "2022-12-02T11:38:10.490616Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-JOflJGBp2T",
    "outputId": "a6f48b42-dfdd-46c3-c9d3-e9fc9534ef61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the implementation is correct, we should expect the function outputs `5`.\n",
    "calculate_receptive_field_size(\n",
    "    simple_model_case_2, x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHNvWsRatedA"
   },
   "source": [
    "# Calculating Receptive Field Size of Neurons from VGG16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:11.460705Z",
     "start_time": "2022-12-02T11:38:10.554619Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161,
     "referenced_widgets": [
      "4c249cb91cb54d86ab3e54ebb7616792",
      "60af9a9991664fb0a3028fe01e83642f",
      "1eb8b1f08c714671b8de975ebd2d4aaf",
      "89347cf76fcc4276bdeaccabb0c82098",
      "a354522a9c7c4c5a8d3257d4a103e713",
      "7f1db550a10a452790f335f6bac6ff21",
      "ecc6c463abdf483a8f774e86c276cbf6",
      "24e53f2a5e7f45cb97171aca7d4412a5",
      "3db7a52bd94b403680701dd70f093598",
      "4139d62768f14413af2f4cee34ab0cb9",
      "9d651248b0604f69b4c2b6abb46617d0"
     ]
    },
    "id": "wGXz41n2tfsD",
    "outputId": "f8b8a17d-7717-4adb-cfd1-e2a95769a603"
   },
   "outputs": [],
   "source": [
    "# We first load a pretrained VGG16 model from the TorchVision model respository.\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# We also set the model to the evaluation mode to prevent\n",
    "# some statistics of the model get updated during forward passes.\n",
    "vgg16 = vgg16.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:11.476702Z",
     "start_time": "2022-12-02T11:38:11.463710Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zy5iRW6d3qyO",
    "outputId": "6a75ddd4-4116-40aa-c42f-440356a19416"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the structure of VGG16\n",
    "vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows that `VGG16` (from PytorchVision) are composed of three modules, namely\n",
    "- `features` mainly contains a sequence of convolution and poolying layers; one can interpret the module as a feature extractor.\n",
    "- `avgpool` is a average pooling over spatial dimensions.\n",
    "- `classifier` is a sequence of linear layers that is responsible to making final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preventing the Quantity of Interest Varnish\n",
    "\n",
    "Under some circumstances, the quantity of our interest could be zero for activation functions like ReLU. From architectures (e.g. `VGG16`) that rely on such activation functions, we therefore have to prevent circumstances to happen.\n",
    "\n",
    "Because we are only interested in computing the quanity (and NOT the output of the model), we can mitigate the effect by replacing ReLU in `VGG16` with the identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:11.491695Z",
     "start_time": "2022-12-02T11:38:11.478696Z"
    },
    "id": "pm-mtAug3R63"
   },
   "outputs": [],
   "source": [
    "def replace_activation_in_module_to_identity(module: torch.nn.Sequential):\n",
    "    \"\"\"\n",
    "    The function replaces all ReLU activation to be identity function.\n",
    "    It prevents the quanity of interest varnish.\n",
    "    \n",
    "    Remark:\n",
    "    In practice, we have to be VERY careful on this type of reassignment.\n",
    "    This is because now we have a different model, and it certianly produces different output from the original one.\n",
    "    For our purpose, this replacement is ok because we do NOT rely directly on the exact output value. \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    for i in range(len(module)):\n",
    "        layer = module[i]\n",
    "    \n",
    "        if isinstance(layer, torch.nn.ReLU):\n",
    "\n",
    "            module[i] = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:11.506696Z",
     "start_time": "2022-12-02T11:38:11.494704Z"
    },
    "id": "uIcZca2gDlmm"
   },
   "outputs": [],
   "source": [
    "replace_activation_in_module_to_identity(vgg16.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:11.522696Z",
     "start_time": "2022-12-02T11:38:11.509712Z"
    },
    "id": "Kz4VoliDDmmr"
   },
   "outputs": [],
   "source": [
    "replace_activation_in_module_to_identity(vgg16.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:11.538699Z",
     "start_time": "2022-12-02T11:38:11.524702Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xiLo2SbZ4aal",
    "outputId": "0d35613a-17a6-4a35-da35-2ce97201c613"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): Identity()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): Identity()\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): Identity()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): Identity()\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): Identity()\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): Identity()\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): Identity()\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): Identity()\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): Identity()\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): Identity()\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): Identity()\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): Identity()\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): Identity()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): Identity()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that ReLU are replaced with Identity\n",
    "vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above, we can see that now all ReLU's are replaced by `Identity()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function to Select Parts of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:11.553698Z",
     "start_time": "2022-12-02T11:38:11.540696Z"
    },
    "id": "0qc4NLdO4bPz"
   },
   "outputs": [],
   "source": [
    "def slice_module(module: torch.nn.Sequential, layer_ix: int) -> torch.nn.Sequential:\n",
    "\n",
    "    \"\"\"\n",
    "    Task 4 (10P): \n",
    "    \n",
    "    The function selects part of the module (torch.nn.Sequential object)\n",
    "    from the first layer (layer_ix=0) to the `layer_ix`-th (inclusive).\n",
    "\n",
    "    Hints:\n",
    "    1. Python starts index at 0;\n",
    "    2. Recall that Python's List Slicing does NOT include the last entry.\n",
    "       For example, [0, 1, 2][:2] is [0, 1].\n",
    "    \"\"\"\n",
    "\n",
    "    import torch.nn as nn\n",
    "\n",
    "    layers = vgg16.features\n",
    "    classifier = vgg16.classifier\n",
    "    \n",
    "    if layer_ix <= 30:\n",
    "        return layers[:layer_ix+1]\n",
    "    elif layer_ix==31:\n",
    "        return layers[:31].add_module(vgg16.avgpool)\n",
    "    else:\n",
    "        before_avg = layers[:31].add_module(vgg16.avgpool)\n",
    "        return before_avg.add_module(classifier[:layer_ix-31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:11.568701Z",
     "start_time": "2022-12-02T11:38:11.555698Z"
    },
    "id": "sKzYLbgm5meN"
   },
   "outputs": [],
   "source": [
    "# Verify that `slice_module` returns an object with correct type.\n",
    "assert isinstance(\n",
    "    slice_module(vgg16.features, 4),\n",
    "    torch.nn.Sequential\n",
    "), \"The function should returns a `torch.nn.Sequential` object.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:11.583695Z",
     "start_time": "2022-12-02T11:38:11.570704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): Identity()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verity that if we slide vgg16.features at layer_ix=1,\n",
    "# we should expect only a convolution layer and its proceeding activation function Identity().\n",
    "slice_module(vgg16.features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:11.598699Z",
     "start_time": "2022-12-02T11:38:11.586707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): Identity()\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): Identity()\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verity that if we slide vgg16.features at layer_ix=4,\n",
    "# we should expect 2x[Conv2d, Identity] and MaxPool2d.\n",
    "slice_module(vgg16.features, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Receptive Field at Various Layer of VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:11.613696Z",
     "start_time": "2022-12-02T11:38:11.600701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Suppose we are interested in these layers.\n",
    "# We assume that these layers are part of vgg16.features.\n",
    "# If not, one has to change the module given to slice_module(...) in the cell below accordingly.\n",
    "LAYERS_OF_INTEREST = [2, 4, 14, 16, 21, 28, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:27.597629Z",
     "start_time": "2022-12-02T11:38:11.615702Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3hXRpkZ6BDE",
    "outputId": "2f967b59-65af-4ef3-b50a-e9932738aa22",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer  2: RF=005 [last-layer=Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\87290\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer  4: RF=006 [last-layer=MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)]\n",
      "Layer 14: RF=040 [last-layer=Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]\n",
      "Layer 16: RF=044 [last-layer=MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)]\n",
      "Layer 21: RF=092 [last-layer=Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]\n",
      "Layer 28: RF=196 [last-layer=Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]\n",
      "Layer 30: RF=212 [last-layer=MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)]\n"
     ]
    }
   ],
   "source": [
    "for layer_ix in LAYERS_OF_INTEREST:\n",
    "    # We slice vgg16.features.\n",
    "    submodule_until_layer_ix = slice_module(vgg16.features, layer_ix)\n",
    "    \n",
    "    # Use the function that we implement to calculate the size of the receptive field.\n",
    "    rf = calculate_receptive_field_size(submodule_until_layer_ix, x)\n",
    "    \n",
    "    \n",
    "    print(f\"Layer {layer_ix:2d}: RF={rf:03.0f} [last-layer={submodule_until_layer_ix[-1]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we see that the receptive field gets larger when we move closer to output layer. This means that neurons in later layers (high-level) see larger parts of the input than neurons in early layers (low-level).\n",
    "\n",
    "Said differently, neurons in early layers can only detect simple or low-level)  features (due to the restrict size of their receptive field. Conversely, neurons in later layers can becomre morespecialize in detecting complex patterns or distinct objects (high-level features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0Q93-o17cPS"
   },
   "source": [
    "# Limitation of the Approach\n",
    "\n",
    "Our current approach on calculating the size of receptive fields requires that the quanity of interest is computable and not varnish.\n",
    "\n",
    "But, in practice, whether the quantity of interest varnishes is sometimes difficult ot realize. This issue leads to an important limitation that one should be aware of.\n",
    "\n",
    "\n",
    "We demonstrate the limitation through a set of examples. In these examples, we look at the receptive field of `VGG16.features`'s Layer 30. From the output, we see that the size of its receptive field is `212`.\n",
    "\n",
    "Instead of using the input with spatial dimensions `224x224`, we vary these dimensions and observe the size of the receptive fields in these configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T11:38:45.860707Z",
     "start_time": "2022-12-02T11:38:27.599633Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9P7pyhqW5C2M",
    "outputId": "086b732a-892f-42f1-ee48-b25486848294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size=(056,056): RF(Layer 30)= 56\n",
      "input size=(098,098): RF(Layer 30)= 98\n",
      "input size=(112,112): RF(Layer 30)=112\n",
      "input size=(224,224): RF(Layer 30)=212\n",
      "input size=(448,448): RF(Layer 30)=212\n"
     ]
    }
   ],
   "source": [
    "submodule_until_layer_30 = slice_module(vgg16.features, 30)\n",
    "\n",
    "for size in [56, 98, 112, 224, 448]:\n",
    "    # Create input with different spatial size\n",
    "    xsmall = torch.randn(10, 3, size, size)\n",
    "\n",
    "    rf = calculate_receptive_field_size(submodule_until_layer_30, xsmall)\n",
    "    \n",
    "    print(f\"input size=({size:03d},{size:03d}): RF(Layer 30)={rf:3.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S_zxXj_73UC"
   },
   "source": [
    "**Task 5 (5 P):** Are the receptive field sizes of Layer 30 the same when using `x` and smaller versions of `x`?\n",
    "\n",
    "If NOT, why and how are they different?\n",
    "\n",
    "*Answer*: ...."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1eb8b1f08c714671b8de975ebd2d4aaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24e53f2a5e7f45cb97171aca7d4412a5",
      "max": 553433881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3db7a52bd94b403680701dd70f093598",
      "value": 553433881
     }
    },
    "24e53f2a5e7f45cb97171aca7d4412a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3db7a52bd94b403680701dd70f093598": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4139d62768f14413af2f4cee34ab0cb9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c249cb91cb54d86ab3e54ebb7616792": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_60af9a9991664fb0a3028fe01e83642f",
       "IPY_MODEL_1eb8b1f08c714671b8de975ebd2d4aaf",
       "IPY_MODEL_89347cf76fcc4276bdeaccabb0c82098"
      ],
      "layout": "IPY_MODEL_a354522a9c7c4c5a8d3257d4a103e713"
     }
    },
    "60af9a9991664fb0a3028fe01e83642f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f1db550a10a452790f335f6bac6ff21",
      "placeholder": "​",
      "style": "IPY_MODEL_ecc6c463abdf483a8f774e86c276cbf6",
      "value": "100%"
     }
    },
    "7f1db550a10a452790f335f6bac6ff21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89347cf76fcc4276bdeaccabb0c82098": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4139d62768f14413af2f4cee34ab0cb9",
      "placeholder": "​",
      "style": "IPY_MODEL_9d651248b0604f69b4c2b6abb46617d0",
      "value": " 528M/528M [00:04&lt;00:00, 139MB/s]"
     }
    },
    "9d651248b0604f69b4c2b6abb46617d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a354522a9c7c4c5a8d3257d4a103e713": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ecc6c463abdf483a8f774e86c276cbf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
